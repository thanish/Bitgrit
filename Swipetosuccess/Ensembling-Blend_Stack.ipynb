{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import gc\n",
    "\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from dask.distributed import Client\n",
    "# from dask import array as da\n",
    "# from dask import dataframe as dd \n",
    "# import dask\n",
    "# from xgboost.dask import DaskDMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903605, 2) (387975, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading the data...\")\n",
    "train_prod = pd.read_pickle(\"../data/train_prod_v9.pickle\")\n",
    "test_prod = pd.read_pickle(\"../data/test_prod_v9.pickle\")\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(train_prod.shape, test_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod['age_difference'] = train_prod['from_age']-train_prod['to_age']\n",
    "test_prod['age_difference'] = test_prod['from_age']-test_prod['to_age']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod.fillna(-999, inplace=True)\n",
    "test_prod.fillna(-999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bottom_importance = [\n",
    " 'from_purpose_id_12',\n",
    " 'to_unique_degree_count',\n",
    " 'from_purpose_id_3',\n",
    " 'from_unique_school_count',\n",
    " 'rev_strength_4',\n",
    " 'to_unique_school_count',\n",
    " 'rev_strength_7',\n",
    " 'rev_strength_8',\n",
    " 'rev_strength_6',\n",
    " 'rev_strength_5']\n",
    "\n",
    "self_intro_columns = train_prod.columns[train_prod.columns.str.contains(\"_self_intro_\")].tolist()\n",
    "\n",
    "to_self_intro_columns = train_prod.columns[train_prod.columns.str.contains(\"to_self_intro_\")].tolist()\n",
    "from_self_intro_columns = train_prod.columns[train_prod.columns.str.contains(\"from_self_intro_\")].tolist()\n",
    "\n",
    "purpose_columns = train_prod.columns[train_prod.columns.str.contains(\"_purpose_id_\")].tolist()\n",
    "rev_strength_columns = train_prod.columns[train_prod.columns.str.contains(\"rev_strength\")].tolist()\n",
    "review_comments = train_prod.columns[train_prod.columns.str.contains(\"_review_comments_\")].tolist()\n",
    "\n",
    "others = ['to_review_comments_count', 'from_review_comments_count', 'to_last_login_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indep length: 2\n",
      "Columns that are dropped: ['from-to']\n"
     ]
    }
   ],
   "source": [
    "dep = 'score'\n",
    "drop = ['from-to', 'user_purpose_cosine_similarity']  + review_comments + rev_strength_columns + from_self_intro_columns\n",
    "indep = train_prod.columns.difference([dep]+drop)\n",
    "\n",
    "print(\"Indep length:\",len(indep))\n",
    "print(\"Columns that are dropped:\", drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_model(prediction_type, train_X, train_Y, test_X, test_Y, test_prod_X):\n",
    "    LR = LogisticRegression()\n",
    "    LR.fit(train_X, train_Y)\n",
    "    \n",
    "    if prediction_type != 'prob':  \n",
    "        LR_local_prediction = LR.predict(test_X)\n",
    "        LR_prod_prediction = LR.predict(test_prod_X)\n",
    "        \n",
    "        print(\"Accuracy:\", accuracy_score(test_Y, LR_local_prediction))\n",
    "    else:\n",
    "        LR_local_prediction = LR.predict_proba(test_X)\n",
    "        LR_prod_prediction = LR.predict_proba(test_prod_X)\n",
    "        \n",
    "        temp = LR_local_prediction.argmax(axis=1)\n",
    "        print(\"Accuracy:\", accuracy_score(test_Y, temp))\n",
    "        \n",
    "    del LR\n",
    "    gc.collect()\n",
    "    \n",
    "    return LR_local_prediction, LR_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_model(prediction_type, train_X, train_Y, test_X, test_Y, test_prod_X):\n",
    "    RF = RandomForestClassifier(n_estimators=20, n_jobs=-1)\n",
    "    RF.fit(train_X, train_Y)\n",
    "    \n",
    "    if prediction_type != 'prob':  \n",
    "        RF_local_prediction = RF.predict(test_X)\n",
    "        RF_prod_prediction = RF.predict(test_prod_X)\n",
    "        \n",
    "        print(\"Accuracy:\", accuracy_score(test_Y, RF_local_prediction))\n",
    "    else:\n",
    "        RF_local_prediction = RF.predict_proba(test_X)\n",
    "        RF_prod_prediction = RF.predict_proba(test_prod_X)\n",
    "    \n",
    "        temp = RF_local_prediction.argmax(axis=1)\n",
    "        print(\"Accuracy:\", accuracy_score(test_Y, temp))\n",
    "        \n",
    "    del RF\n",
    "    gc.collect()\n",
    "    \n",
    "    return RF_local_prediction, RF_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBM_model(prediction_type, train_X, train_Y, test_X, test_Y, test_prod_X):\n",
    "    GBM = GradientBoostingClassifier(n_estimators=20)\n",
    "    GBM.fit(train_X, train_Y)\n",
    "\n",
    "    if prediction_type != 'prob':  \n",
    "        GBM_local_prediction = GBM.predict(test_X)\n",
    "        GBM_prod_prediction = GBM.predict(test_prod_X)\n",
    "        \n",
    "        print(\"Accuracy:\", accuracy_score(test_Y, GBM_local_prediction))\n",
    "    else:\n",
    "        GBM_local_prediction = GBM.predict_proba(test_X)\n",
    "        GBM_prod_prediction = GBM.predict_proba(test_prod_X)\n",
    "        \n",
    "        temp = GBM_local_prediction.argmax(axis=1)\n",
    "        print(\"Accuracy:\", accuracy_score(test_Y, temp))\n",
    "    \n",
    "    del GBM\n",
    "    gc.collect()\n",
    "    \n",
    "    return GBM_local_prediction, GBM_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_model(prediction_type, train_X, train_Y, test_X, test_Y, test_prod_X):\n",
    "    nrounds = 10000\n",
    "\n",
    "    eval_dataset = Pool(test_X, test_Y)\n",
    "    np.random.seed(100)\n",
    "    cat_local_model = CatBoostClassifier(iterations=nrounds\n",
    "                                         ,learning_rate=0.4\n",
    "                                         ,depth=13\n",
    "                                         #,subsample=0.8\n",
    "                                         #,colsample_bylevel=1\n",
    "                                         ,task_type=\"CPU\"\n",
    "                                         #,loss_function='RMSE'\n",
    "                                         ,eval_metric='Accuracy'\n",
    "                                         ,early_stopping_rounds=20\n",
    "                                             ,verbose=False\n",
    "                                        )\n",
    "\n",
    "    cat_local_model.fit(train_X, train_Y,\n",
    "                        eval_set=eval_dataset)\n",
    "    \n",
    "    print(\"Best Iteration:\", cat_local_model.best_iteration_)\n",
    "    print(\"Best Accuracy:\", cat_local_model.best_score_['validation']['Accuracy'])\n",
    "    \n",
    "    if prediction_type != 'prob':\n",
    "        cat_local_prediction = cat_local_model.predict(test_X).reshape(-1)\n",
    "        cat_prod_prediction = cat_local_model.predict(test_prod_X).reshape(-1)\n",
    "        \n",
    "    else:\n",
    "        cat_local_prediction = cat_local_model.predict_proba(test_X)\n",
    "        cat_prod_prediction = cat_local_model.predict_proba(test_prod_X)\n",
    "        \n",
    "    del eval_dataset, cat_local_model\n",
    "    gc.collect()\n",
    "    \n",
    "    return cat_local_prediction, cat_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_eval_accuracy(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = preds.reshape(len(np.unique(labels)), -1)\n",
    "    preds = preds.argmax(axis = 0)\n",
    "    acc = accuracy_score(y_pred = preds, y_true = labels)\n",
    "    return 'Accuracy', acc, True\n",
    "\n",
    "def lgb_model(prediction_type, train_X, train_Y, test_X, test_Y, test_prod_X):\n",
    "    \n",
    "    num_rounds = 10000\n",
    "    \n",
    "    lgb_train_local = lgb.Dataset(train_X, train_Y, free_raw_data=False)\n",
    "    lgb_test_local = lgb.Dataset(test_X, test_Y, reference=lgb_train_local,  free_raw_data=False)\n",
    "\n",
    "    #lgb_test_prod = lgb.Dataset(test_prod[indep], reference=lgb_train_prod)\n",
    "\n",
    "    params = {\n",
    "    #     'device_type':'gpu',\n",
    "        'nthreads':12,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class':4,\n",
    "        'metric': 'custom',\n",
    "        'num_leaves': 150,\n",
    "        #'max_depth': 10,\n",
    "        'learning_rate': 0.03,\n",
    "        'feature_fraction': 0.6,\n",
    "        'bagging_fraction': 1,\n",
    "        'bagging_freq': 1,\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "    np.random.seed(100)\n",
    "    lgb_model_local = lgb.train(params,\n",
    "                                lgb_train_local,\n",
    "                                num_boost_round=num_rounds ,\n",
    "                                valid_sets=lgb_test_local,\n",
    "                                feval=lgb_eval_accuracy,\n",
    "                                early_stopping_rounds=20,\n",
    "                                verbose_eval=False)\n",
    "    \n",
    "    print(\"LGB Best iteration:\", lgb_model_local.best_iteration)\n",
    "    print(\"LGB Best Accuracy:\", lgb_model_local.best_score['valid_0']['Accuracy'])\n",
    "    \n",
    "    if prediction_type != 'prob':\n",
    "        lgb_local_prediction = lgb_model_local.predict(test_X).argmax(axis=1)\n",
    "        lgb_prod_prediction = lgb_model_local.predict(test_prod_X).argmax(axis=1)\n",
    "    else:\n",
    "        lgb_local_prediction = lgb_model_local.predict(test_X)\n",
    "        lgb_prod_prediction = lgb_model_local.predict(test_prod_X)\n",
    "\n",
    "    del lgb_train_local, lgb_test_local, lgb_model_local\n",
    "    gc.collect()\n",
    "    \n",
    "    return lgb_local_prediction, lgb_prod_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_eval_accuracy(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    acc = accuracy_score(y_pred = preds, y_true = labels)\n",
    "    return 'Accuracy', acc\n",
    "\n",
    "def XGB_model(prediction_type, train_X, train_Y, test_X, test_Y, test_prod_X):\n",
    "    \n",
    "    dtrain_local = xgb.DMatrix(data = train_X, label = train_Y)\n",
    "    dtest_local = xgb.DMatrix(data = test_X, label = test_Y)\n",
    "    \n",
    "    dtest_prod = xgb.DMatrix(data = test_prod_X)\n",
    "\n",
    "    eval_set = [(dtrain_local,'train'), (dtest_local,'test')]\n",
    "\n",
    "    num_rounds = 10000\n",
    "    \n",
    "    params = {'objective' : 'multi:softprob'\n",
    "              ,'num_class' : 4\n",
    "              #,'eval_metric': 'rmse'\n",
    "              ,'max_depth' : 6\n",
    "              ,'eta' : 0.2\n",
    "              ,'subsample': 1\n",
    "              ,'colsample_bytree': 1\n",
    "              #,'tree_method' : 'gpu_hist'\n",
    "              }\n",
    "\n",
    "    np.random.seed(100)\n",
    "    xgb_model_local = xgb.train(params,\n",
    "                                dtrain_local,\n",
    "                                evals = eval_set,\n",
    "                                num_boost_round = num_rounds,\n",
    "                                #feval = xgb_eval_accuracy,\n",
    "                                #maximize = True,\n",
    "                                verbose_eval = False,\n",
    "                                early_stopping_rounds = 30)\n",
    "\n",
    "          \n",
    "    if prediction_type != 'prob':\n",
    "        xgb_local_prediction = xgb_model_local.predict(dtest_local).argmax(axis=1)\n",
    "        xgb_prod_prediction = xgb_model_local.predict(dtest_prod).argmax(axis=1)        \n",
    "        print(\"XGB Best accuracy:\", accuracy_score(xgb_local_prediction, test_Y))\n",
    "    else:\n",
    "        xgb_local_prediction = xgb_model_local.predict(dtest_local)\n",
    "        xgb_prod_prediction = xgb_model_local.predict(dtest_prod)\n",
    "        print(\"XGB Best accuracy:\", accuracy_score(xgb_local_prediction.argmax(axis=1), test_Y))\n",
    "\n",
    "    print(\"XGB Best iteration:\", xgb_model_local.best_iteration)\n",
    "    \n",
    "    del dtrain_local, dtest_local, dtest_prod, eval_set, xgb_model_local\n",
    "    gc.collect()\n",
    "    \n",
    "    return xgb_local_prediction, xgb_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "nfolds = 5\n",
    "uniq_classes = train_prod['score'].nunique()\n",
    "kf = KFold(n_splits=nfolds, shuffle=True, random_state=100)\n",
    "\n",
    "prediction_type = 'prob'\n",
    "\n",
    "LR_chosen = True\n",
    "RF_chosen = False\n",
    "GBM_chosen = False\n",
    "CAT_chosen = True\n",
    "LGB_chosen = True\n",
    "XGB_chosen = True\n",
    "\n",
    "print(f\"Running {nfolds} validation\")\n",
    "print(f\"The prediction type chosen is {prediction_type}\")\n",
    "\n",
    "if prediction_type == 'prob':\n",
    "    if LR_chosen:\n",
    "        LR_fold=np.empty(shape=(0, uniq_classes))\n",
    "        LR_prod=np.empty(shape=(test_prod.shape[0], uniq_classes))\n",
    "\n",
    "    if RF_chosen:\n",
    "        RF_fold=np.empty(shape=(0, uniq_classes))\n",
    "        RF_prod=np.empty(shape=(test_prod.shape[0], uniq_classes))\n",
    "\n",
    "    if GBM_chosen:\n",
    "        GBM_fold=np.empty(shape=(0, uniq_classes))\n",
    "        GBM_prod=np.empty(shape=(test_prod.shape[0], uniq_classes))\n",
    "    \n",
    "    if CAT_chosen:\n",
    "        CAT_fold=np.empty(shape=(0, uniq_classes))\n",
    "        CAT_prod=np.empty(shape=(test_prod.shape[0], uniq_classes))\n",
    "    \n",
    "    if LGB_chosen:\n",
    "        LGB_fold=np.empty(shape=(0, uniq_classes))\n",
    "        LGB_prod=np.empty(shape=(test_prod.shape[0], uniq_classes))\n",
    "\n",
    "    if XGB_chosen:\n",
    "        XGB_fold=np.empty(shape=(0, uniq_classes))\n",
    "        XGB_prod=np.empty(shape=(test_prod.shape[0], uniq_classes))\n",
    "        \n",
    "else:\n",
    "    if LR_chosen:\n",
    "        LR_fold=np.empty(shape=(0, 1))\n",
    "        LR_prod=np.empty(shape=(test_prod.shape[0], 1))\n",
    "\n",
    "    if RF_chosen:\n",
    "        RF_fold=np.empty(shape=(0, 1))\n",
    "        RF_prod=np.empty(shape=(test_prod.shape[0], 1))\n",
    "\n",
    "    if GBM_chosen:\n",
    "        GBM_fold=np.empty(shape=(0, 1))\n",
    "        GBM_prod=np.empty(shape=(test_prod.shape[0], 1))\n",
    "    \n",
    "    if CAT_chosen:\n",
    "        CAT_fold=np.empty(shape=(0, 1))\n",
    "        CAT_prod=np.empty(shape=(test_prod.shape[0], 1))\n",
    "    \n",
    "    if LGB_chosen:\n",
    "        LGB_fold=np.empty(shape=(0, 1))\n",
    "        LGB_prod=np.empty(shape=(test_prod.shape[0], 1))\n",
    "    \n",
    "    if XGB_chosen:\n",
    "        XGB_fold=np.empty(shape=(0, 1))\n",
    "        XGB_prod=np.empty(shape=(test_prod.shape[0], 1))\n",
    "\n",
    "train_prod_stacked_score = []\n",
    "train_prod_stack_from_to = []\n",
    "for i, (train_local_index, test_local_index) in enumerate(kf.split(train_prod[indep])):\n",
    "    \n",
    "    train_local_X, train_local_Y = train_prod.loc[train_local_index, indep], train_prod.loc[train_local_index, dep]\n",
    "    test_local_X, test_local_Y = train_prod.loc[test_local_index, indep], train_prod.loc[test_local_index, dep]\n",
    "    \n",
    "    train_prod_stacked_score = train_prod_stacked_score + test_local_Y.values.tolist()\n",
    "    train_prod_stack_from_to = train_prod_stack_from_to + train_prod['from-to'][test_local_index].values.tolist()\n",
    "    \n",
    "    print(\"##################################################################################################################\")\n",
    "    print(\"Current Fold:\", i)\n",
    "    print(\"\")\n",
    "    \n",
    "    if LR_chosen:\n",
    "        print(\"Training the Logistic Regression model\")\n",
    "        LR_fold_prediction, LR_prod_prediction = LogisticRegression_model(prediction_type, train_X=train_local_X, train_Y=train_local_Y, test_X=test_local_X, test_Y=test_local_Y, test_prod_X=test_prod[indep])\n",
    "        \n",
    "        if prediction_type == 'prob':\n",
    "            LR_fold = np.append(LR_fold, LR_fold_prediction, axis=0)\n",
    "            LR_prod = np.sum([LR_prod, LR_prod_prediction], axis=0)\n",
    "            #print(\"LR shape\", LR_fold.shape, LR_prod.shape)\n",
    "        else:\n",
    "            LR_fold = np.append(LR_fold, LR_fold_prediction.reshape(-1, 1), axis=0)\n",
    "            #LR_prod = np.append(LR_prod, LR_prod_prediction.reshape(-1, 1), axis=1)\n",
    "            LR_prod = np.sum([LR_prod, LR_prod_prediction.reshape(-1, 1)], axis=0)\n",
    "            #print(\"LR shape\", LR_fold.shape, LR_prod.shape)\n",
    "    \n",
    "        del LR_fold_prediction, LR_prod_prediction\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\")\n",
    "    if RF_chosen:\n",
    "        print(\"Training the RandomForest model\")\n",
    "        RF_fold_prediction, RF_prod_prediction = RandomForest_model(prediction_type, train_X=train_local_X, train_Y=train_local_Y, test_X=test_local_X, test_Y=test_local_Y, test_prod_X=test_prod[indep])\n",
    "    \n",
    "        if prediction_type == 'prob':    \n",
    "            RF_fold = np.append(RF_fold, RF_fold_prediction, axis=0)\n",
    "            RF_prod = np.sum([RF_prod, RF_prod_prediction], axis=0)\n",
    "            #print(\"RF shape\", RF_fold.shape, RF_prod.shape)\n",
    "        else:\n",
    "            RF_fold = np.append(RF_fold, RF_fold_prediction.reshape(-1, 1), axis=0)\n",
    "            #RF_prod = np.append(RF_prod, RF_prod_prediction.reshape(-1, 1), axis=1)\n",
    "            RF_prod = np.sum([RF_prod, RF_prod_prediction.reshape(-1, 1)], axis=0)\n",
    "            #print(\"RF shape\", RF_fold.shape, RF_prod.shape)\n",
    "\n",
    "        del RF_fold_prediction, RF_prod_prediction\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"\")\n",
    "    if GBM_chosen:\n",
    "        print(\"Training the GBM model\")\n",
    "        GBM_fold_prediction, GBM_prod_prediction = GBM_model(prediction_type, train_X=train_local_X, train_Y=train_local_Y, test_X=test_local_X, test_Y=test_local_Y, test_prod_X=test_prod[indep])\n",
    "\n",
    "        if prediction_type == 'prob':\n",
    "            GBM_fold = np.append(GBM_fold, GBM_fold_prediction, axis=0)\n",
    "            GBM_prod = np.sum([GBM_prod, GBM_prod_prediction], axis=0)\n",
    "            #print(\"GBM shape\", GBM_fold.shape, GBM_prod.shape)\n",
    "        else:\n",
    "            GBM_fold = np.append(GBM_fold, GBM_fold_prediction.reshape(-1, 1), axis=0)\n",
    "            #GBM_prod = np.append(GBM_prod, GBM_prod_prediction.reshape(-1, 1), axis=1)\n",
    "            GBM_prod = np.sum([GBM_prod, GBM_prod_prediction.reshape(-1, 1)], axis=0)\n",
    "            #print(\"GBM shape\", GBM_fold.shape, GBM_prod.shape)\n",
    "\n",
    "        del GBM_fold_prediction, GBM_prod_prediction\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"\")\n",
    "    if CAT_chosen:\n",
    "        print(\"Training the CAT model\")\n",
    "        CAT_fold_prediction, CAT_prod_prediction = catboost_model(prediction_type, train_X=train_local_X, train_Y=train_local_Y, test_X=test_local_X, test_Y=test_local_Y, test_prod_X=test_prod[indep])\n",
    "\n",
    "        if prediction_type == 'prob':\n",
    "            CAT_fold = np.append(CAT_fold, CAT_fold_prediction, axis=0)\n",
    "            CAT_prod = np.sum([CAT_prod, CAT_prod_prediction], axis=0)\n",
    "            #print(\"CAT shape\", CAT_fold.shape, CAT_prod.shape)\n",
    "        else:\n",
    "            CAT_fold = np.append(CAT_fold, CAT_fold_prediction.reshape(-1, 1), axis=0)\n",
    "            #CAT_prod = np.append(CAT_prod, CAT_prod_prediction.reshape(-1, 1), axis=1)\n",
    "            CAT_prod = np.sum([CAT_prod, CAT_prod_prediction.reshape(-1, 1)], axis=0)\n",
    "            #print(\"CAT shape\", CAT_fold.shape, CAT_prod.shape)\n",
    "\n",
    "        del CAT_fold_prediction, CAT_prod_prediction\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\")\n",
    "    if LGB_chosen:\n",
    "        print(\"Training the LGB model\")\n",
    "        LGB_fold_prediction, LGB_prod_prediction = lgb_model(prediction_type, train_X=train_local_X, train_Y=train_local_Y, test_X=test_local_X, test_Y=test_local_Y, test_prod_X=test_prod[indep])\n",
    "\n",
    "        if prediction_type == 'prob':\n",
    "            LGB_fold = np.append(LGB_fold, LGB_fold_prediction, axis=0)\n",
    "            LGB_prod = np.sum([LGB_prod, LGB_prod_prediction], axis=0)\n",
    "            #print(\"LGB shape\", LGB_fold.shape, LGB_prod.shape)\n",
    "        else:\n",
    "            LGB_fold = np.append(LGB_fold, LGB_fold_prediction.reshape(-1, 1), axis=0)\n",
    "            #LGB_prod = np.append(LGB_prod, LGB_prod_prediction.reshape(-1, 1), axis=1)\n",
    "            LGB_prod = np.sum([LGB_prod, LGB_prod_prediction.reshape(-1, 1)], axis=0)\n",
    "            #print(\"LGB shape\", LGB_fold.shape, LGB_prod.shape)\n",
    "\n",
    "        del LGB_fold_prediction, LGB_prod_prediction\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"\")\n",
    "    if XGB_chosen:\n",
    "        print(\"Training the XGB model\")\n",
    "        XGB_fold_prediction, XGB_prod_prediction = XGB_model(prediction_type, train_X=train_local_X, train_Y=train_local_Y, test_X=test_local_X, test_Y=test_local_Y, test_prod_X=test_prod[indep])\n",
    "\n",
    "        if prediction_type == 'prob':\n",
    "            XGB_fold = np.append(XGB_fold, XGB_fold_prediction, axis=0)\n",
    "            XGB_prod = np.sum([XGB_prod, XGB_prod_prediction], axis=0)\n",
    "            #print(\"XGB shape\", XGB_fold.shape, XGB_prod.shape)\n",
    "        else:\n",
    "            XGB_fold = np.append(XGB_fold, XGB_fold_prediction.reshape(-1, 1), axis=0)\n",
    "            #XGB_prod = np.append(XGB_prod, XGB_prod_prediction.reshape(-1, 1), axis=1)\n",
    "            XGB_prod = np.sum([XGB_prod, XGB_prod_prediction.reshape(-1, 1)], axis=0)\n",
    "            #print(\"XGB shape\", XGB_fold.shape, XGB_prod.shape)\n",
    "\n",
    "        del XGB_fold_prediction, XGB_prod_prediction\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"##################################################################################################################\")\n",
    "    \n",
    "if LR_chosen:\n",
    "    LR_prod = LR_prod/nfolds\n",
    "    col_names = [\"LR_class_\" + str(i) for i in range(uniq_classes)]\n",
    "    LR_train_stack = pd.DataFrame(LR_fold, columns=col_names)\n",
    "    LR_test_stack = pd.DataFrame(LR_prod, columns=col_names)\n",
    "\n",
    "    del LR_fold, LR_prod\n",
    "    gc.collect()\n",
    "    \n",
    "if RF_chosen:\n",
    "    RF_prod = RF_prod/nfolds\n",
    "    col_names = [\"RF_class_\" + str(i) for i in range(uniq_classes)]\n",
    "    RF_train_stack = pd.DataFrame(RF_fold, columns=col_names)\n",
    "    RF_test_stack = pd.DataFrame(RF_prod, columns=col_names)\n",
    "\n",
    "    del RF_fold, RF_prod\n",
    "    gc.collect()\n",
    "    \n",
    "if GBM_chosen:\n",
    "    GBM_prod = GBM_prod/nfolds\n",
    "    col_names = [\"GBM_class_\" + str(i) for i in range(uniq_classes)]\n",
    "    GBM_train_stack = pd.DataFrame(GBM_fold, columns=col_names)\n",
    "    GBM_test_stack = pd.DataFrame(GBM_prod, columns=col_names)\n",
    "\n",
    "    del GBM_fold, GBM_prod\n",
    "    gc.collect()\n",
    "    \n",
    "if CAT_chosen:\n",
    "    CAT_prod = CAT_prod/nfolds\n",
    "    col_names = [\"CAT_class_\" + str(i) for i in range(uniq_classes)]\n",
    "    CAT_train_stack = pd.DataFrame(CAT_fold, columns=col_names)\n",
    "    CAT_test_stack = pd.DataFrame(CAT_prod, columns=col_names)\n",
    "\n",
    "    del CAT_fold, CAT_prod\n",
    "    gc.collect()\n",
    "    \n",
    "if LGB_chosen:\n",
    "    LGB_prod = LGB_prod/nfolds    \n",
    "    col_names = [\"LGB_class_\" + str(i) for i in range(uniq_classes)]\n",
    "    LGB_train_stack = pd.DataFrame(LGB_fold, columns=col_names)\n",
    "    LGB_test_stack = pd.DataFrame(LGB_prod, columns=col_names)\n",
    "    \n",
    "    del LGB_fold, LGB_prod\n",
    "    gc.collect()\n",
    "\n",
    "if XGB_chosen:\n",
    "    XGB_prod = XGB_prod/nfolds    \n",
    "    col_names = [\"XGB_class_\" + str(i) for i in range(uniq_classes)]\n",
    "    XGB_train_stack = pd.DataFrame(XGB_fold, columns=col_names)\n",
    "    XGB_test_stack = pd.DataFrame(XGB_prod, columns=col_names)\n",
    "    \n",
    "    del XGB_fold, XGB_prod\n",
    "    gc.collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_stacked_df(stack_list):\n",
    "    temp = pd.DataFrame()\n",
    "    for df in stack_list:\n",
    "        temp = pd.concat([temp, df], axis=1)\n",
    "    return temp\n",
    "\n",
    "train_prod_list = [#LR_train_stack, RF_train_stack, GBM_train_stack, \n",
    "                   CAT_train_stack, LGB_train_stack, XGB_train_stack]\n",
    "test_prod_list = [#LR_test_stack, #RF_test_stack, GBM_test_stack, \n",
    "                  CAT_test_stack, LGB_test_stack, XGB_test_stack]\n",
    "\n",
    "train_prod_stack = get_final_stacked_df(stack_list=train_prod_list)\n",
    "test_prod_stack = get_final_stacked_df(stack_list=test_prod_list)\n",
    "\n",
    "del CAT_train_stack, LGB_train_stack, XGB_train_stack\n",
    "del CAT_test_stack, LGB_test_stack, XGB_test_stack\n",
    "gc.collect()\n",
    "\n",
    "train_prod_stack['from-to'] = train_prod_stack_from_to\n",
    "test_prod_stack['from-to'] = test_prod['from-to']\n",
    "\n",
    "train_prod_stack['score'] = train_prod_stacked_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forming the stacked dataset complete\")\n",
    "print(\"#############################################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR_class_0</th>\n",
       "      <th>LR_class_1</th>\n",
       "      <th>LR_class_2</th>\n",
       "      <th>LR_class_3</th>\n",
       "      <th>CAT_class_0</th>\n",
       "      <th>CAT_class_1</th>\n",
       "      <th>CAT_class_2</th>\n",
       "      <th>CAT_class_3</th>\n",
       "      <th>LGB_class_0</th>\n",
       "      <th>LGB_class_1</th>\n",
       "      <th>LGB_class_2</th>\n",
       "      <th>LGB_class_3</th>\n",
       "      <th>XGB_class_0</th>\n",
       "      <th>XGB_class_1</th>\n",
       "      <th>XGB_class_2</th>\n",
       "      <th>XGB_class_3</th>\n",
       "      <th>from-to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.352273</td>\n",
       "      <td>0.507783</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.130571</td>\n",
       "      <td>5.382390e-01</td>\n",
       "      <td>4.205111e-01</td>\n",
       "      <td>1.234213e-02</td>\n",
       "      <td>2.890775e-02</td>\n",
       "      <td>0.448086</td>\n",
       "      <td>0.507931</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.036046</td>\n",
       "      <td>0.474764</td>\n",
       "      <td>0.375599</td>\n",
       "      <td>0.069657</td>\n",
       "      <td>0.079980</td>\n",
       "      <td>217075-4685538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.548561</td>\n",
       "      <td>0.406225</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>3.134227e+06</td>\n",
       "      <td>3.136086e+06</td>\n",
       "      <td>3.136508e+06</td>\n",
       "      <td>1.117196e+05</td>\n",
       "      <td>0.912853</td>\n",
       "      <td>0.522936</td>\n",
       "      <td>0.013222</td>\n",
       "      <td>0.050989</td>\n",
       "      <td>2.203321</td>\n",
       "      <td>1.274404</td>\n",
       "      <td>0.241674</td>\n",
       "      <td>0.280601</td>\n",
       "      <td>6244786-1445883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.252783</td>\n",
       "      <td>0.252753</td>\n",
       "      <td>0.244735</td>\n",
       "      <td>0.249729</td>\n",
       "      <td>3.134535e+06</td>\n",
       "      <td>1.609551e+04</td>\n",
       "      <td>3.146760e+06</td>\n",
       "      <td>4.076247e+05</td>\n",
       "      <td>0.865267</td>\n",
       "      <td>0.555013</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.060112</td>\n",
       "      <td>2.009685</td>\n",
       "      <td>1.433352</td>\n",
       "      <td>0.243509</td>\n",
       "      <td>0.313453</td>\n",
       "      <td>23911-18259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.255452</td>\n",
       "      <td>0.256785</td>\n",
       "      <td>0.237771</td>\n",
       "      <td>0.249992</td>\n",
       "      <td>3.135066e+06</td>\n",
       "      <td>7.287505e+05</td>\n",
       "      <td>3.157483e+06</td>\n",
       "      <td>3.158061e+06</td>\n",
       "      <td>0.662424</td>\n",
       "      <td>0.737433</td>\n",
       "      <td>0.025373</td>\n",
       "      <td>0.074770</td>\n",
       "      <td>0.830553</td>\n",
       "      <td>2.648030</td>\n",
       "      <td>0.219623</td>\n",
       "      <td>0.301794</td>\n",
       "      <td>17459-78547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.279815</td>\n",
       "      <td>0.280140</td>\n",
       "      <td>0.195095</td>\n",
       "      <td>0.244950</td>\n",
       "      <td>2.478498e+06</td>\n",
       "      <td>3.110418e+06</td>\n",
       "      <td>1.669032e+06</td>\n",
       "      <td>2.717058e+06</td>\n",
       "      <td>0.798428</td>\n",
       "      <td>0.627707</td>\n",
       "      <td>0.017496</td>\n",
       "      <td>0.056369</td>\n",
       "      <td>1.933084</td>\n",
       "      <td>1.586854</td>\n",
       "      <td>0.206946</td>\n",
       "      <td>0.273116</td>\n",
       "      <td>250404-220746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387970</th>\n",
       "      <td>0.255418</td>\n",
       "      <td>0.257040</td>\n",
       "      <td>0.237430</td>\n",
       "      <td>0.250112</td>\n",
       "      <td>5.435568e-01</td>\n",
       "      <td>1.958267e+00</td>\n",
       "      <td>2.164610e-01</td>\n",
       "      <td>2.817151e-01</td>\n",
       "      <td>0.638127</td>\n",
       "      <td>0.766966</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.071678</td>\n",
       "      <td>0.759899</td>\n",
       "      <td>2.779913</td>\n",
       "      <td>0.196460</td>\n",
       "      <td>0.263728</td>\n",
       "      <td>11081-87146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387971</th>\n",
       "      <td>0.551897</td>\n",
       "      <td>0.401093</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.045454</td>\n",
       "      <td>2.237595e+00</td>\n",
       "      <td>5.572369e-01</td>\n",
       "      <td>6.211370e-02</td>\n",
       "      <td>1.430545e-01</td>\n",
       "      <td>0.993614</td>\n",
       "      <td>0.439979</td>\n",
       "      <td>0.010651</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>2.886819</td>\n",
       "      <td>0.711750</td>\n",
       "      <td>0.157910</td>\n",
       "      <td>0.243521</td>\n",
       "      <td>6274366-1238849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387972</th>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.395840</td>\n",
       "      <td>0.030757</td>\n",
       "      <td>0.149330</td>\n",
       "      <td>2.199391e+00</td>\n",
       "      <td>5.255323e-01</td>\n",
       "      <td>7.942211e-02</td>\n",
       "      <td>1.956547e-01</td>\n",
       "      <td>0.942218</td>\n",
       "      <td>0.486644</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>2.952625</td>\n",
       "      <td>0.635011</td>\n",
       "      <td>0.158758</td>\n",
       "      <td>0.253606</td>\n",
       "      <td>2324651-1067329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387973</th>\n",
       "      <td>0.334416</td>\n",
       "      <td>0.405805</td>\n",
       "      <td>0.054335</td>\n",
       "      <td>0.205444</td>\n",
       "      <td>2.246880e+00</td>\n",
       "      <td>4.588826e-01</td>\n",
       "      <td>9.617525e-02</td>\n",
       "      <td>1.980619e-01</td>\n",
       "      <td>1.027588</td>\n",
       "      <td>0.401354</td>\n",
       "      <td>0.018629</td>\n",
       "      <td>0.052429</td>\n",
       "      <td>3.105410</td>\n",
       "      <td>0.530016</td>\n",
       "      <td>0.162889</td>\n",
       "      <td>0.201685</td>\n",
       "      <td>41869-2419226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387974</th>\n",
       "      <td>0.564542</td>\n",
       "      <td>0.373508</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.058220</td>\n",
       "      <td>1.469121e+00</td>\n",
       "      <td>1.284855e+00</td>\n",
       "      <td>8.486144e-02</td>\n",
       "      <td>1.611627e-01</td>\n",
       "      <td>0.771128</td>\n",
       "      <td>0.661722</td>\n",
       "      <td>0.011321</td>\n",
       "      <td>0.055828</td>\n",
       "      <td>1.849525</td>\n",
       "      <td>1.639518</td>\n",
       "      <td>0.219116</td>\n",
       "      <td>0.291842</td>\n",
       "      <td>6276312-83049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387975 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LR_class_0  LR_class_1  LR_class_2  LR_class_3   CAT_class_0  \\\n",
       "0         0.352273    0.507783    0.009373    0.130571  5.382390e-01   \n",
       "1         0.548561    0.406225    0.001360    0.043854  3.134227e+06   \n",
       "2         0.252783    0.252753    0.244735    0.249729  3.134535e+06   \n",
       "3         0.255452    0.256785    0.237771    0.249992  3.135066e+06   \n",
       "4         0.279815    0.280140    0.195095    0.244950  2.478498e+06   \n",
       "...            ...         ...         ...         ...           ...   \n",
       "387970    0.255418    0.257040    0.237430    0.250112  5.435568e-01   \n",
       "387971    0.551897    0.401093    0.001557    0.045454  2.237595e+00   \n",
       "387972    0.424073    0.395840    0.030757    0.149330  2.199391e+00   \n",
       "387973    0.334416    0.405805    0.054335    0.205444  2.246880e+00   \n",
       "387974    0.564542    0.373508    0.003730    0.058220  1.469121e+00   \n",
       "\n",
       "         CAT_class_1   CAT_class_2   CAT_class_3  LGB_class_0  LGB_class_1  \\\n",
       "0       4.205111e-01  1.234213e-02  2.890775e-02     0.448086     0.507931   \n",
       "1       3.136086e+06  3.136508e+06  1.117196e+05     0.912853     0.522936   \n",
       "2       1.609551e+04  3.146760e+06  4.076247e+05     0.865267     0.555013   \n",
       "3       7.287505e+05  3.157483e+06  3.158061e+06     0.662424     0.737433   \n",
       "4       3.110418e+06  1.669032e+06  2.717058e+06     0.798428     0.627707   \n",
       "...              ...           ...           ...          ...          ...   \n",
       "387970  1.958267e+00  2.164610e-01  2.817151e-01     0.638127     0.766966   \n",
       "387971  5.572369e-01  6.211370e-02  1.430545e-01     0.993614     0.439979   \n",
       "387972  5.255323e-01  7.942211e-02  1.956547e-01     0.942218     0.486644   \n",
       "387973  4.588826e-01  9.617525e-02  1.980619e-01     1.027588     0.401354   \n",
       "387974  1.284855e+00  8.486144e-02  1.611627e-01     0.771128     0.661722   \n",
       "\n",
       "        LGB_class_2  LGB_class_3  XGB_class_0  XGB_class_1  XGB_class_2  \\\n",
       "0          0.007936     0.036046     0.474764     0.375599     0.069657   \n",
       "1          0.013222     0.050989     2.203321     1.274404     0.241674   \n",
       "2          0.019608     0.060112     2.009685     1.433352     0.243509   \n",
       "3          0.025373     0.074770     0.830553     2.648030     0.219623   \n",
       "4          0.017496     0.056369     1.933084     1.586854     0.206946   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "387970     0.023228     0.071678     0.759899     2.779913     0.196460   \n",
       "387971     0.010651     0.055756     2.886819     0.711750     0.157910   \n",
       "387972     0.013615     0.057524     2.952625     0.635011     0.158758   \n",
       "387973     0.018629     0.052429     3.105410     0.530016     0.162889   \n",
       "387974     0.011321     0.055828     1.849525     1.639518     0.219116   \n",
       "\n",
       "        XGB_class_3          from-to  \n",
       "0          0.079980   217075-4685538  \n",
       "1          0.280601  6244786-1445883  \n",
       "2          0.313453      23911-18259  \n",
       "3          0.301794      17459-78547  \n",
       "4          0.273116    250404-220746  \n",
       "...             ...              ...  \n",
       "387970     0.263728      11081-87146  \n",
       "387971     0.243521  6274366-1238849  \n",
       "387972     0.253606  2324651-1067329  \n",
       "387973     0.201685    41869-2419226  \n",
       "387974     0.291842    6276312-83049  \n",
       "\n",
       "[387975 rows x 17 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_prod_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing the stacked train and test dataset\")\n",
    "\n",
    "train_prod_stack.to_pickle(\"../data/train_prod_stack.pickle\")\n",
    "test_prod_stack.to_pickle(\"../data/test_prod_stack.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod_stack = pd.read_pickle(\"../data/train_prod_stack.pickle\")\n",
    "test_prod_stack = pd.read_pickle(\"../data/test_prod_stack.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the dataset for the stacked model\n",
      "(722884, 12) (722884,) (180721, 12) (180721,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Creating the dataset for the stacked model\")\n",
    "dep='score'\n",
    "indep = train_prod_stack.columns.difference(['from-to', dep])\n",
    "\n",
    "np.random.seed(100)\n",
    "train_stack_local_X, test_stack_local_X, train_stack_local_Y, test_stack_local_Y = train_test_split(train_prod_stack[indep],\n",
    "                                                                                                    train_prod_stack[dep], \n",
    "                                                                                                    test_size=0.2,\n",
    "                                                                                                    stratify=train_prod_stack[dep])\n",
    "\n",
    "print(train_stack_local_X.shape, train_stack_local_Y.shape, test_stack_local_X.shape, test_stack_local_Y.shape)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data for the light gbm meta model\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating data for the light gbm meta model\")\n",
    "lgb_train_stack_local = lgb.Dataset(train_stack_local_X, train_stack_local_Y, free_raw_data=False)\n",
    "lgb_test_stack_local = lgb.Dataset(test_stack_local_X, test_stack_local_Y, reference=lgb_train_stack_local,  free_raw_data=False)\n",
    "\n",
    "lgb_train_stack_prod = lgb.Dataset(train_prod_stack[indep], train_prod_stack[dep])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "#     'device_type':'gpu',\n",
    "    'nthreads':8,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':4,\n",
    "    'metric': 'custom',\n",
    "    'num_leaves': 150,\n",
    "    #'max_depth': 10,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 1,\n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 1,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "num_rounds = 10000\n",
    "print('Starting training...')\n",
    "start = datetime.now()\n",
    "\n",
    "np.random.seed(100)\n",
    "lgb_model_stack_local = lgb.train(params,\n",
    "                                  lgb_train_stack_local,\n",
    "                                  num_boost_round=num_rounds,\n",
    "                                  valid_sets=lgb_test_stack_local,\n",
    "                                  feval=lgb_eval_accuracy,\n",
    "                                  early_stopping_rounds=50)\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"\")\n",
    "print(\"Total training time:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prod model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_round = lgb_model_stack_local.best_iteration + int(lgb_model_stack_local.best_iteration*0.3)\n",
    "\n",
    "print(\"Validation rounds:\", lgb_model_stack_local.best_iteration)\n",
    "print(\"Final round is:\", final_round)\n",
    "\n",
    "print('Starting training...')\n",
    "start = datetime.now()\n",
    "\n",
    "np.random.seed(100)\n",
    "lgb_model_prod = lgb.train(params,\n",
    "                            lgb_train_stack_prod,\n",
    "                            num_boost_round=final_round ,\n",
    "                            valid_sets=lgb_test_stack_local,\n",
    "                            feval=lgb_eval_accuracy,\n",
    "#                             early_stopping_rounds=20\n",
    "                          )\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"\")\n",
    "print(\"Total training time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the prediction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Running the prediction\")\n",
    "lgb_stack_prod_prediction = lgb_model_stack_local.predict(test_prod_stack[indep])\n",
    "lgb_stack_prod_prediction = lgb_stack_prod_prediction.argmax(axis=1)\n",
    "lgb_stack_prod_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the prediction to the folder\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing the prediction to the folder\")\n",
    "\n",
    "lgb_submission = pd.DataFrame({\"from-to\": test_prod_stack['from-to'],\n",
    "                               \"score\": lgb_stack_prod_prediction.astype('float')})\n",
    "\n",
    "lgb_submission.to_csv(\"../submissions/lgb_stack_sub_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
