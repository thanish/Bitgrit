{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'batch_size' : 5096,\n",
    "          'num_worker' : 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod = pd.read_pickle(\"../data/train_prod_v15.pickle\")\n",
    "test_prod = pd.read_pickle(\"../data/test_prod_v15.pickle\")\n",
    "\n",
    "print(train_prod.shape, test_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Find the age difference\")\n",
    "\n",
    "train_prod['age_difference'] = train_prod['from_age']-train_prod['to_age']\n",
    "test_prod['age_difference'] = test_prod['from_age']-test_prod['to_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod['to_swipe_by_session_percentage'] = (train_prod['to_total_swipe_counts']/ train_prod['to_total_session_count']).replace(np.inf, 0)\n",
    "train_prod['from_swipe_by_session_percentage'] = (train_prod['from_total_swipe_counts']/ train_prod['to_total_session_count']).replace(np.inf, 0)\n",
    "\n",
    "test_prod['to_swipe_by_session_percentage'] = (test_prod['to_total_swipe_counts']/ test_prod['to_total_session_count']).replace(np.inf, 0)\n",
    "test_prod['from_swipe_by_session_percentage'] = (test_prod['from_total_swipe_counts']/ test_prod['to_total_session_count']).replace(np.inf, 0)\n",
    "\n",
    "train_prod['to_common_users_left_swipe_percentage'] = (train_prod['common_users_swiped_left']/train_prod['to_swipe_left_count']).replace(np.inf, 0)\n",
    "train_prod['from_common_users_left_swipe_percentage'] = (train_prod['common_users_swiped_left']/train_prod['from_swipe_left_count']).replace(np.inf, 0)\n",
    "\n",
    "train_prod['to_common_users_right_swipe_percentage'] = (train_prod['common_users_swiped_right']/train_prod['to_swipe_right_count']).replace(np.inf, 0)\n",
    "train_prod['from_common_users_right_swipe_percentage'] = (train_prod['common_users_swiped_right']/train_prod['from_swipe_right_count']).replace(np.inf, 0)\n",
    "\n",
    "train_prod['to_overall_common_users_left_swipe_percentage'] = (train_prod['common_users_swiped_left']/train_prod['to_total_swipe_counts']).replace(np.inf, 0)\n",
    "train_prod['from_overall_common_users_left_swipe_percentage'] = (train_prod['common_users_swiped_left']/train_prod['from_total_swipe_counts']).replace(np.inf, 0)\n",
    "\n",
    "train_prod['to_overall_common_users_right_swipe_percentage'] = (train_prod['common_users_swiped_right']/train_prod['to_total_swipe_counts']).replace(np.inf, 0)\n",
    "train_prod['from_overall_common_users_right_swipe_percentage'] = (train_prod['common_users_swiped_right']/train_prod['from_total_swipe_counts']).replace(np.inf, 0)\n",
    "\n",
    "test_prod['to_common_users_left_swipe_percentage'] = (test_prod['common_users_swiped_left']/test_prod['to_swipe_left_count']).replace(np.inf, 0)\n",
    "test_prod['from_common_users_left_swipe_percentage'] = (test_prod['common_users_swiped_left']/test_prod['from_swipe_left_count']).replace(np.inf, 0)\n",
    "\n",
    "test_prod['to_common_users_right_swipe_percentage'] = (test_prod['common_users_swiped_right']/test_prod['to_swipe_right_count']).replace(np.inf, 0)\n",
    "test_prod['from_common_users_right_swipe_percentage'] = (test_prod['common_users_swiped_right']/test_prod['from_swipe_right_count']).replace(np.inf, 0)\n",
    "\n",
    "test_prod['to_overall_common_users_left_swipe_percentage'] = (test_prod['common_users_swiped_left']/test_prod['to_total_swipe_counts']).replace(np.inf, 0)\n",
    "test_prod['from_overall_common_users_left_swipe_percentage'] = (test_prod['common_users_swiped_left']/test_prod['from_total_swipe_counts']).replace(np.inf, 0)\n",
    "\n",
    "test_prod['to_overall_common_users_right_swipe_percentage'] = (test_prod['common_users_swiped_right']/test_prod['to_total_swipe_counts']).replace(np.inf, 0)\n",
    "test_prod['from_overall_common_users_right_swipe_percentage'] = (test_prod['common_users_swiped_right']/test_prod['from_total_swipe_counts']).replace(np.inf, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod = train_prod.replace(999999, 0) \n",
    "test_prod = test_prod.replace(999999, 0) \n",
    "train_prod.fillna(0, inplace=True)\n",
    "test_prod.fillna(0, inplace=True)\n",
    "\n",
    "print(train_prod.shape, test_prod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_bottom_importance = [\n",
    "                         'from_purpose_id_12',\n",
    "                         'to_unique_degree_count',\n",
    "                         'from_purpose_id_3',\n",
    "                         'from_unique_school_count',\n",
    "                         'rev_strength_4',\n",
    "                         'to_unique_school_count',\n",
    "                         'rev_strength_7',\n",
    "                         'rev_strength_8',\n",
    "                         'rev_strength_6',\n",
    "                         'rev_strength_5']\n",
    "\n",
    "self_intro_columns = train_prod.columns[train_prod.columns.str.contains(\"_self_intro_\")].tolist()\n",
    "\n",
    "to_self_intro_columns = train_prod.columns[train_prod.columns.str.contains(\"to_self_intro_\")].tolist()\n",
    "from_self_intro_columns = train_prod.columns[train_prod.columns.str.contains(\"from_self_intro_\")].tolist()\n",
    "\n",
    "purpose_columns = train_prod.columns[train_prod.columns.str.contains(\"_purpose_id_\")].tolist()\n",
    "rev_strength_columns = train_prod.columns[train_prod.columns.str.contains(\"rev_strength\")].tolist()\n",
    "review_comments = train_prod.columns[train_prod.columns.str.contains(\"_review_comments_\")].tolist()\n",
    "\n",
    "others = ['to_review_comments_count', 'from_review_comments_count', 'to_last_login_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = 'score'\n",
    "drop = ['from-to', 'user_purpose_cosine_similarity', 'to_last_swipe_year']  + review_comments + rev_strength_columns # + from_self_intro_columns\n",
    "indep = train_prod.columns.difference([dep]+drop)\n",
    "\n",
    "print(\"Indep length:\",len(indep))\n",
    "print(\"Columns that are dropped:\", drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_prod[indep])\n",
    "\n",
    "train_prod[indep] = scaler.transform(train_prod[indep])\n",
    "test_prod[indep] = scaler.transform(test_prod[indep])\n",
    "\n",
    "train_prod.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "train_local_X, test_local_X, train_local_Y, test_local_Y = train_test_split(train_prod[indep],\n",
    "                                                                            train_prod[dep], \n",
    "                                                                            test_size=0.2,\n",
    "                                                                            stratify=train_prod[dep])\n",
    "\n",
    "train_local_X, test_local_X, train_local_Y, test_local_Y = train_local_X.reset_index(drop=True), test_local_X.reset_index(drop=True), train_local_Y.reset_index(drop=True), test_local_Y.reset_index(drop=True)\n",
    "print(train_local_X.shape, train_local_Y.shape, test_local_X.shape, test_local_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_features, num_labels):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(self.num_features, 2056)\n",
    "        self.hidden_layer_2 = nn.Linear(2056, 2056)\n",
    "        self.output = nn.Linear(2056, self.num_labels)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(2056)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(2056)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.hidden_layer_1(X)\n",
    "        out = self.batch_norm_1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.hidden_layer_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "                \n",
    "        out = self.hidden_layer_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "                \n",
    "        out = self.hidden_layer_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "                \n",
    "        out = self.hidden_layer_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "                \n",
    "        out = self.hidden_layer_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "                \n",
    "        out = self.hidden_layer_2(out)\n",
    "        out = self.batch_norm_2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.output(out)\n",
    "        #out = nn.Softmax()(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormInputs():\n",
    "    def __init__(self, features, label, datatype='train'):\n",
    "        self.datatype=datatype\n",
    "        self.features = features\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.features.loc[index, :]\n",
    "        \n",
    "        if self.datatype=='test':\n",
    "            Y = 1\n",
    "        else:\n",
    "            Y = self.label[index]\n",
    "        \n",
    "        return {'features' : torch.tensor(X, dtype=torch.float),\n",
    "                'target' : torch.tensor(Y, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_local_input = FormInputs(features=train_local_X, label=train_local_Y, datatype='train')\n",
    "test_local_input = FormInputs(features=test_local_X, label=test_local_Y, datatype='train')\n",
    "train_prod_input = FormInputs(features=train_prod[indep], label=train_prod[dep], datatype='train')\n",
    "test_prod_input = FormInputs(features=test_prod[indep], label=None, datatype='test')\n",
    "\n",
    "test_prod_input[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing the data to data loader\n",
    "train_local_data_loader = torch.utils.data.DataLoader(train_local_input,\n",
    "                                                      shuffle=True,\n",
    "                                                      batch_size=config['batch_size'],\n",
    "                                                      num_workers=config['num_worker'])\n",
    "test_local_data_loader = torch.utils.data.DataLoader(test_local_input,\n",
    "                                                     shuffle=True,\n",
    "                                                     batch_size=config['batch_size'],\n",
    "                                                     num_workers=config['num_worker'])\n",
    "train_prod_data_loader = torch.utils.data.DataLoader(train_prod_input,\n",
    "                                                     shuffle=True,\n",
    "                                                     batch_size=config['batch_size'],\n",
    "                                                     num_workers=config['num_worker'])\n",
    "test_prod_data_loader = torch.utils.data.DataLoader(test_prod_input,\n",
    "                                                    shuffle=True,\n",
    "                                                    batch_size=config['batch_size'],\n",
    "                                                    num_workers=config['num_worker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting_seed(seed_no):\n",
    "    random.seed(seed_no)\n",
    "    np.random.seed(seed_no)\n",
    "    torch.manual_seed(seed_no)\n",
    "    torch.cuda.manual_seed_all(seed_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, data_loader):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    final_loss = 0\n",
    "    for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        feature = data['features'].to(device)\n",
    "        target = data['target'].to(device)\n",
    "        \n",
    "        prediction = model(feature)\n",
    "        \n",
    "        prediction_soft_max = nn.Softmax(dim=1)(prediction)\n",
    "        prediction_soft_max = prediction_soft_max.argmax(axis=1)\n",
    "        #print(\"train_actual:\", np.unique(target.detach().cpu().numpy(), return_counts=True))\n",
    "        #print(\"train_predicted:\", np.unique(prediction_soft_max.detach().cpu().numpy(), return_counts=True))\n",
    "        \n",
    "        loss = loss_func(prediction, target)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        final_loss += loss\n",
    "    \n",
    "    final_loss = final_loss/len(data_loader)\n",
    "    \n",
    "    return model, final_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, data_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    final_loss = 0\n",
    "    actual_output=[]\n",
    "    predicted_output=[]\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(data_loader), total = len(data_loader)):\n",
    "            feature = data['features'].to(device)\n",
    "            target = data['target'].to(device)\n",
    "\n",
    "            prediction = model(feature)\n",
    "                        \n",
    "            loss = loss_func(prediction, target)\n",
    "            final_loss += loss\n",
    "            \n",
    "            #print(\"validation_prediction:\", prediction)\n",
    "            \n",
    "            prediction = nn.Softmax(dim=1)(prediction)\n",
    "            \n",
    "            #print(\"softmax_prediction:\", prediction)\n",
    "            \n",
    "            prediction = prediction.argmax(axis=1)\n",
    "            \n",
    "            #print(\"Argmax_prediction:\", prediction)\n",
    "            \n",
    "            predicted_output.extend(prediction.detach().cpu().numpy().tolist())\n",
    "            actual_output.extend(target.detach().cpu().numpy().tolist())\n",
    "                        \n",
    "        print(\"Predicted output:\", np.unique(predicted_output, return_counts=True))\n",
    "        print(\"Actual output:\", np.unique(actual_output, return_counts=True))\n",
    "        print(confusion_matrix(y_true=actual_output, y_pred=predicted_output))\n",
    "        \n",
    "        final_loss = final_loss/len(data_loader)\n",
    "        accuracy = accuracy_score(predicted_output, actual_output)\n",
    "    \n",
    "    return final_loss, actual_output, predicted_output, accuracy       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(model, data_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    predicted_output=[]\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(data_loader), total = len(data_loader)):\n",
    "            feature = data['features'].to(device)\n",
    "            \n",
    "            prediction = model(feature)\n",
    "                        \n",
    "            prediction = nn.Softmax(dim=1)(prediction)\n",
    "            prediction = prediction.argmax(axis=1)\n",
    "            \n",
    "            predicted_output.extend(prediction.detach().cpu().numpy().tolist())\n",
    "                        \n",
    "    return predicted_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_engine(epochs, train_data, eval_data, patience):\n",
    "    \n",
    "    setting_seed(seed_no=100)\n",
    "    model = MulticlassClassification(num_features=len(indep), num_labels=4)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "        \n",
    "    counter = 0\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        model, train_loss = train_fn(model, optimizer, data_loader=train_data)\n",
    "        eval_loss, eval_actual, eval_prediction, accuracy = eval_fn(model, data_loader=eval_data)\n",
    "        \n",
    "        print(\"Epoch: {} train loss: {} eval loss: {} eval accuracy {}\".format(epoch, train_loss, eval_loss, accuracy))\n",
    "        \n",
    "        if accuracy > best_accuracy:  \n",
    "        \n",
    "            best_accuracy = accuracy\n",
    "            counter = 0\n",
    "\n",
    "            model_path = '../saved_model/best_model_1.bin'            \n",
    "            print(\"Saving the model:\", model_path)\n",
    "            torch.save(model, model_path)\n",
    "            \n",
    "        else:\n",
    "            counter += 1\n",
    "            print(\"Patience:\", counter)\n",
    "            \n",
    "            if counter == patience:\n",
    "                print(\"Reached the patience threshold so ending the training\")\n",
    "                break            \n",
    "        \n",
    "        print(\"Best Accuracy:\", best_accuracy)\n",
    "        \n",
    "    return model, eval_actual, eval_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, eval_actual, eval_prediction = train_engine(epochs = 500, \n",
    "                                                   train_data=train_local_data_loader, \n",
    "                                                   eval_data=test_local_data_loader,\n",
    "                                                   patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_save_path = '../saved_model/saved_state'\n",
    "loaded_state = torch.load(state_save_path)\n",
    "\n",
    "model = MulticlassClassification(num_features=len(indep), num_labels=4)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "model_weight = model.load_state_dict(loaded_state['model_state_dict'])\n",
    "model_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
